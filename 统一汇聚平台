



统一汇聚平台
	1 文件分发模块
		1.1 目录监控
			要求:(1)可配置监控目录
			     (2)优先处理最新文件
				 (3)根据管理平台配置的表,只扫描表对应的文件,其他文件移动到错误目录或者删除(可配).
		1.2 文件解析
			要求:(1)根据管理平台配置的表的分隔符&换行符解析文件,解析为一条条记录,统一将每条记录的分割符替换为^
				 (2)日志记录解析的文件名以及文件名对应的记录数,方便问题核查和责任追究
				 (3)性能要高,解析线程数可配
				 (4)不用解析具体字段,统一替换每一条记录的分隔符为^
		1.3 数据发送
			要求:(1)为每张表自动创建KAFKA Topic,Topic名字取表名字, 统一小写, 每张表默认自动创建10个分区2个副本,存在就不管
			     (2)根据表名, 作为生产端,将数据写入kafka topic
				 (3)将写入量定期记录入日志, 方便问题核查和责任追究, 并且提取1条记录作为样例数据写入日志, 方便看记录时间
	2 数据入库模块
		2.1 数据接收
			要求:(1)作为消费端, 根据管理平台配置的表, 在kafka中消费对应的topic
			     (2)将消费量定期记录入日志, 方便问题核查和责任追究, 并且提取1条记录作为样例数据写入日志, 方便看记录时间
		2.2 记录解析
			要求:(1)可以配置表的分区时间字段,默认是endtime,procdure_endtime,procedureendtime,time 可统一配置.
			     (2)可配置记录的过期时间
			     (3)将解析量定期记录入日志, 方便问题核查和责任追究, 记录下每个表的过期记录条数, 如果有过期记录,给出一条样例记录
		2.2 Hive数据处理
			要求:(1)将解析后的记录, 根据分区时间, 在临时目录写parquet文件, 单个文件记录数/文件大小/超时时间到了后, 完成文件生成
			     (2)除了超时时间生成文件外, 其他方式生成文件需要避免小文件生成
				 (3)生成的文件名/大小/记录数 写入日志,方便问题核查和责任追究
				 (4)文件生成后, put方式写入HDFS
		2.3 HBase数据处理
			要求:(1)可配置通过常昊bulkload方式入hbase
			     (2)可配置通过put/flushcommit方式入hbase
				 (3)入hbase量定期记录入日志, 方便问题核查和责任追究
		2.4 预统功能
			要求:(1)支持老入库分发程序的流统SQL和关联配置表
			     (2)不进行group by操作, 直接根据记录记录和统计条件生成流统结果, 不要缓存结果!!!!
				 (3)将流统结果, 根据分区时间, 在临时目录写parquet文件, 单个文件记录数/文件大小/超时时间到了后, 完成文件生成
				 (4)生成的文件名/大小/记录数 写入日志,方便问题核查和责任追究
				 (5)文件生成后, put方式写入HDFS